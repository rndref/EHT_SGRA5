\section{Observational Constraints}\label{sec:observations}

\sgra is one of the most observed objects in the sky.  It has been observed with a slew of telescopes, across 5 decades in time and more than 17 decades in frequency. We need to select a manageable subset of this data to constrain our models. In doing so we have attempted to select (1) approximately uncorrelated constraints, so that each tests a distinct aspect of the model; (2) constraints based on data that can be simulated with the models; (3) constraints based on EHT 2017 1.3mm VLBI data or based on photons produced within or close to the 1.3mm emission region that are contemporaneous or near-contemporaneous.

Variability complicates the comparison of Sgr A* models to the data. Even an ideal evolution of a successful model will never match the data point for point because the space of possible realizations of the model is large; the dataset is high dimensional.  We expect the {\em distribution} of  observables derived from a successful model to match the data, however, if the model is run long enough to sample the distribution.  For example, the distribution of the $86$GHz flux density $F_{86}$ for a model ought to contain the observed $F_{86}$.

Most observables are correlated over $\tau \sim $few $\times 100 G M/c^3$.  To obtain the mean and variance of the model distribution to accuracy $f$ from a GRMHD model, then, requires running it for $\sim \tau/f^2 \sim 30,000 (\tau/300) (f/0.1)^{-2}$, assuming the model decorrelates completely at intervals $\gg \tau$.  A GRMHD model of duration $30,000 \tg$  between $10^3-10^4$ node-hours, depending on resolution, code, and hardware.  This is expensive since $\sim 10$ runs are used for each model set, and runs need to be repeated for multiple values of numerical parameters (e.g. resolution).  Most of our models are run for $30,000 \tg$; a few are run to $100,000 \tg$.  The model distributions therefore contain sampling noise, and this must be accounted for in model selection.  For some  constraints (e.g. m-ring fits; see below) this can be done by sampling the model at a cadence similar to the correlation time and then using a 2-sample KS test to compare the observed and model distributions.

%==============================================================================
% \subsection{Scattering Models}

%==============================================================================
\subsection{EHT Observational Constraints}

\ckc{ck's first pass}
\mw{I think it would be good to mention the EHT array composition (name the telescopes)}
\cfg{much of this material could be incorporated by reference to paperII}

\begin{figure*}
  \centering
  %\includegraphics{}
  \note{altex: (left) visibility amplitude vs baseline for the day(s) that
    this study use, overplotted by the visibility amplitude from a
    fiducial model.
    Similar to paper~II, figure~7.
    Visually mark null location constraint, pre-imaging size (i.e.,
    second moment).
    (right) SED from Gurther overplotted by the SEDs from a fiducial
    model.
    Visually mark the SED constraints.}
  \caption{(\emph{left}) Measured correlated flux densities of \sgra
    on April 7, 2017, from the HOPS pipeline, overplotted with a fiducial
    GRMHD+GRRT model.
    Details on the data can be found in paper~II, section~5.
    A description of the fiducial model is in section~\ref{sec:models}.
    (\emph{right}) \ckc{Q: do we want to show only EHT observation and
      referencing to other papers for non-EHT constraints?
      Or should we have representative figures for all measurements?}}
  \label{fig:visibility}
\end{figure*}

The EHT observed \sgra at 1.3 mm during the 2017 April 5--11 observing campaign and obtained horizon scale complex visibilities.  Figure~\ref{fig:visibility} shows the visibility amplitudes on April 6 and 7 from  the HOPS pipeline, overplotted with visibility amplitudes derived from a model.  Evidently there are nulls near $\sim 5\mathrm{G}\lambda$ and $7\mathrm{G}\lambda$, suggesting a ringlike structure.\monika{mention already here is the shown data is descattered?}

We test the models against EHT interferometric data in three ways.  First, we compare the location of the first null and the visibility amplitude at long baselines to the model (``null constraint'').  Second, we compare an estimate of the source size (``second moment constraint'') to an estimate from short baselines.  Finally, using a variant of the procedure used in \citetalias{PaperIV}, we compare fits for the diameter, width, and asymmetry of an m-ring to the model.

%------------------------------------------------------------------------------
\subsubsection{230\,GHz VLBI Null Locations}

\ckc{Feryal and CK's first pass}

One way we characterize the GRMHD simulations carried out for
different black hole and flow parameters and assess their
compatibility with the \sgra\ data involves an analysis in the visibility
domain.
Figure~\ref{fig:visibility} shows two example snapshots and the
corresponding visibility amplitudes (VA) as a function of baseline
length for a vertical and horizontal image cross sections from a
simulation.
Even though the locations and depths of the minima in the visibility
amplitudes are primarily set by the image size, which is set by the
black hole mass, this figure shows that they can exhibit significant
variability from snapshot to snapshot because of the multitude of
structures that originate in the turbulent flow (Medeiros et
al. 2018).
In particular, the minima tend to move to larger baselines as the ring
thickness temporarily changes in response to, e.g., the appearance of
a flux tube or a similar structure.
In addition, images that have a higher degree of azimuthal asymmetry
show pronounced differences in their vertical and horizontal cross
sections.

The degree of VA variability is different from model to model and also
depends on some of the global characteristics of the flow.
For example, the overall electron density in the disk plays a role by
its effect on the ring thickness: thicker rings show more change in
the visibility minima between snapshots than thinner ones (see also
Satapathy et al. 2021 for the effect on closure phases).
Because of this, while no model is expected to resemble the observed
visibility amplitude data 100\% of the time, it is nevertheless
possible as well as discriminating to require an agreement between the
locations of the minima in each simulation snapshot and the visibility
amplitude data from \sgra\ a reasonable fraction of the time.

To carry out the comparison with the data, we focus on the VA observed
on 2017 April 7, \#3599 because that night has the best u-v coverage
near the minima.\monika{I think the first sentence should be mentioned
in the first paragraph in section 2.1}
The first visibility minima in both the N-S and E-W directions occur
between $2.5-3.5$\;G$\lambda$, as we show in
Figure~\ref{fig:cmp_null}.
We define compliance for a snapshot by requiring that the VA obtained
for {\it either} the horizontal {\it or} the vertical cross section of
a snapshot have a minimum in this range of baseline lengths.

A second feature of the visibility amplitudes that can help discern
between models is the behavior of the VA at long baselines.
April 7 data also show that the amplitudes have declined to $<6\%$ of
the zero baseline flux at baselines between $6-8$\;G$\lambda$ along
all orientations.
This is characteristic of ring-like images with a relatively high
degree of azimuthal symmetry.
Images that are more asymmetric, on the other hand, lead to
significantly higher amplitudes at baselines much larger than the
first minimum, or no minima at all, as the lower panels in
Figure~\ref{fig:cmp_VA} illustrate.
As a result, selecting models based on how frequently they produce
snapshots with such large-baseline power helps identify models that
are in accordance with the data.

One consideration when comparing models to data at long baselines is
the effect of interstellar scattering.
Diffractive scattering has the effect of convolving the image with a
smooth kernel and can reduce the amplitudes to $\sim 70\%$ of their
intrinsic value in the $6-8$\;G$\lambda$ range (REF).
Refractive scattering, on the other hand, introduces a noise at these
baselines of the order of $0.5-3\%$, depending on the particular
characteristics of the scattering screen toward \sgra\ (REF).
To account for both of these effects, we choose a $6\%$ upper limit to
visibility amplitudes from a model snapshot when defining compliance,
as we show in Figure~\ref{fig:cmp_null}.

We assign a compliance fraction to a simulation based on the fraction
of snapshots that pass both criteria we described above.
We will discuss the results of this comparison in the next section,
along with the other model scoring criteria.

%------------------------------------------------------------------------------
\subsubsection{230\,GHz VLBI Pre-Image Size}

\ckc{ck's first pass}

The second moment of an EHT source corresponds, in the $(u,v)$ domain, to
the second derivative of the visibility amplitude near zero baseline length.
That is, the 2nd moment tensor
\begin{equation}
    \sigma_{ij} \equiv \int \, d^2x\, x_i x_j I/\int \, d^2x \, I = (2\pi)^2 \left(\partial_i \partial_j \tilde{I}\right)/\tilde{I}
\end{equation}
where $I$ is the intensity, $\tilde{I}$ its Fourier transform, $i =
x,y$ on the left and $i = u,v$ on the right, and the terms on the
right are evaluated at $u = v = 0$.
Depending on the structure of the visilbiity amplitudes, a pair of visibility amplitudes ($|\tilde{I}|$) on short baselines or
the zero baseline flux can therefore be used to estimate the second moments of the
image.

Without assuming a ring prior, this procedure is used in EHT3 \citetalias{PaperIII} to set an upper limit of $95\mu$as on
the scattered source size and lower limit of $38\mu$as.
Note that, if we used the prior information that there is a ring and a null, the constraint can be narrower.

In evaluating a model we find which images in a model could lie within the allowed band, that is, have minor axis FWHM smaller than the upper limit or major axis FWHM larger than the lower limit.  The fraction of images $f$ that lies within the allowed band is the {\em compliance fraction}.  We pass models with $f > 0.01$.

%------------------------------------------------------------------------------
\subsubsection{230\,GHz M-Ring Fitting}

How can we extract information about the spatial structure of the source from noisy, fluctuating data with limited UV coverage, and compare that to noisy, fluctuating models? Our strategy is to fit a source-plane model to the data and summarize the source structure using the distribution of fit parameters.

\citetalias{PaperIV} fits an ``m-ring'' source model to the April 7 data.  Here we use a simplified m-ring model: a $\delta$ function in radius with diameter $d$ multiplied by a truncated (for $m > 3$) Fourier series, convolved with a Gaussian of width $w$.  In addition the model contains a centered Gaussian component, with amplitude and width as free parameters, to absorb large scale emission and emission interior to the ring.

The simplified m-ring model has 10 parameters; 3 are commonly well constrained and physically interpretable and are therefore used here: the m-ring diameter $d$, the m-ring width $w$ (FWHM of the convolved Gaussian), and the $m=1$ relative amplitude $\beta_1$.

For the comparison dataset we selected 10 120s scans spread approximately uniformly through the ``best-times'' region on 7 April.  They are separated by an average of $\simeq 1240\sec \simeq 60 \tg$. The selected scans have > 10 baselines, and integration time at all stations $> 40s$.  We chose 10 scans to limit computational cost.   Only modest changes in model selection were observed if any one scan was removed from the comparison.  The data were de-scattered, that is, the visibility amplitudes were divided by the scattering kernel.  Maximum likelihood m-ring parameters were then found for each scan.

The fits are listed in Table \ref{tab:mringfits}.  Evidently the fit parameters are noisy.  The fit for $d$ from $39\mu$as to $84\mu$as, for $w$ from $9\mu$as to $21\mu$as, and for $\beta_1$ from $0.04$ to $0.48$ ($\beta_1 < 0.5$ is sufficient for positivity of the model image).

\begin{deluxetable}{ccccc}
\tablecaption{M-Ring Fits to EHT Observations}
\tablehead{ %
\colhead{Scan \#} & %
\colhead{t [UTC hrs]} & %
\colhead{d [$\mu$as]} & %
\colhead{w [$\mu$as] } & %
\colhead{$\beta_1$} %
}
\startdata
111 & 11.28 & 83.87 & 8.87  & 0.122 \\
121 & 11.78 & 57.09 & 13.98 & 0.220 \\
125 & 11.92 & 55.63 & 16.46 & 0.132 \\
130 & 12.35 & 40.68 & 19.08 & 0.039 \\
134 & 12.62 & 57.22 & 17.22 & 0.368 \\
142 & 12.92 & 58.80 & 17.55 & 0.208 \\
149 & 13.28 & 52.31 & 21.16 & 0.278 \\
155 & 13.75 & 38.94 & 18.17 & 0.482 \\
163 & 14.05 & 56.22 & 19.86 & 0.470 \\
171 & 14.38 & 39.48 & 17.71 & 0.408 \\
\enddata
\end{deluxetable}
\label{tab:mringfits}

The variation in fit parameters is caused by a combination of variability in the source, thermal noise, uncertain in the position angle, and gain variations.  In the models the main driver of fit variations is variability in the models themselves.

Next, we read in a series of model images, generated synthetic data from the models, and fit m-rings to the synthetic data.  This produces a distribution of m-ring parameters for each model.

The synthetic data is generated as follows.  A model image $I(x,y)$ is fourier transformed to complex visibilities $V(u,v)$ with an assumed position angle, then sampled on baselines $i$ drawn from the comparison scan, $V_i \equiv V(u_i,v_i)$.  Normally distributed thermal noise $\delta V_{th,i}$, with amplitude based on telescope performance during the scan is added, and multiplicative, normally distributed noise $N$ is added to (very crudely) model gain corrections: $\tilde{V}_i = V_i (1 + \epsilon N) + \delta V_{th,i}$.  We set $\epsilon = 0.05$, but no significant changes in fit parameters were observed for $\epsilon = 0.02$.  We then fit to the visibility amplitudes $|\tilde{V}_i|$ and closure phases arg$(\tilde{V}_i \tilde{V}_i \tilde{V}_j \tilde{V}_k^*)$, where $ijk$ form a triangle in the $(u,v)$ plane.  The procedure was repeated for 4 positions angles and for a set of model images separated by $500 \tg$ (comparable to a correlation time).  This results in, for example, a sample of $30$ fits per scan per position angle for the Illinois thermal model set, or a total of $30 \times 10 = 300$ samples in each distribution.

In comparing the models to the data we (1) generate the distribution of fit parameters at each position angle; (2) run a Kolmogorov-Smirnov test comparing the distribution of $300$ fits to synthetic data with the distribution of $10$ fits to the observations, to obtain a p-value (what is the probability they are drawn from the same underlying distribution?); (3) average p-values over position angles; (4) reject the model if $p < 0.01$.

%==============================================================================
\subsection{Non-EHT Constraints}

In addition to the EHT data, the SED of \sgra is well constrained \citetalias{PaperII} and thus potentially constraining for the models.  We limit comparison to three bands: 86GHz VLBI data; $2.2\mu$m flux; and X-ray luminosity.

%------------------------------------------------------------------------------
\subsubsection{86\,GHz Flux}

The Global Millimeter VLBI Array (GMVA) observed \sgra on 3 April 2017, just 3 days ($\approx 13,000 \tg$) before the EHT campaign.  \cite{2019ApJ...871...30I} estimate that the compact flux is $F_{86}=2.0 \pm 0.2$Jy ($2\sigma$ errors).

We compute 86GHz images for all models, and from that the 86GHz flux density $F_{86}$.  This implicitly assumes that the 86GHz variability over $\sim 3$days is well sampled during the model integration.  We assume normally distributed errors with $\sigma = 0.1$Jy, and convolve the $F_{86}$ distribution for each model with the resulting Gaussian.  We reject models with CDF $< 1\%$ or $> 99\%$ at $2.0$Jy.
\monika{do we need to split 86 GHz into two separate sub-sections?}
\subsubsection{86\,GHz Image Size}

The GMVA observations from 3 April 2017 also constrain the FWHM of the source major axis ${\rm FWHM}_{maj} = 146^{+11}_{-12}\mu$as \citep[95\% confidence][]{2021ApJ...915...99I}.

We compute 86GHz images for all models, and for every model image compute the major axis FWHM.  We assume normally distributed errors with $\sigma = 6\mu$as, and convolve the model distribution with the resulting Gaussian.  We reject models with CDF $< 1\%$ or $> 99\%$ at $146\mu$as.

%------------------------------------------------------------------------------
\subsubsection{NIR (Non-Overproduction) Constraints}

\sgra flares in the near infrared a few times per day, and in 2017 had a median $2.2\mu$m flux density $\simeq 1.0$mJy \citep{2020A&A...638A...2G}.

We compute $2.2\mu$m images for all models.  We reject the model if its median flux exceeds the observed median flux.

There is as yet no generally accepted model for the NIR flares, so we accept models that do not produce flares. Our working hypothesis is that models that underproduce NIR and which do not flare can be saved by accelerating a small fraction of the electron population into a nonthermal tail.

%------------------------------------------------------------------------------
\subsubsection{X-ray (Non-Overproduction) Constraints}

\sgra flares in the X-ray less about once per day.  Chandra observations during the 2017 campaign suggest an upper limit on the median (quiescent) flux of $10^{33}$erg/s.

We compute SEDs for most models.  We reject the model if its median flux exceeds the upper limit on the quiescent flux.

As for the NIR, we do not require that the model produce X-ray flares.

%==============================================================================
\subsection{Time Dependence}

% * we can model ``long-term climate'' well but not ``long-term weather''.
% * Hence we only compare the slowly varying quantities such as mean/median 2nd moments (climate) but not modulation index (weather).
% * TODO: look up correct statistical terminology in weather/climate modeling and build that this into this paper.
% * Use review: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019EA000586 , https://ui.adsabs.harvard.edu/abs/1993RvMP...65.1331A/abstract
%
% * multi-time scale
% * feeding at large and slow scale
% * time scale much shorter when the plasma reach horizon
% * at 1.3mm, we only see the flow near the horizon
% * a "hidden variable" problem that there're slowly varying "boundary conditions" that control the nature of the accretion flow
% * but we only see the fast varying flow
% * Given the boundary conditions (environment), GRMHD + GRRT is very successfully in predicting the time average images.  (Refer to consistent checks in appendex)
% * At the short time scale, the system is chaotic and sensittive to initial conditions.
% * We reach order-of-magnitude agreement.  But in terms of constraint they should not be used.
% * For the date we use, EHT observations show lower variability compared to what we expected.

\cfg{I think we should just describe the observations here.}
\ckc{We don't have a discussion section on variability yet.  Maybe under 5.7 or 5.8?}

\sgra shows variability on a wide range of timescales.  This is expected: there are likely variations induced by fluctuations in stellar wind feeding at the scale of the S-stars down to the turbulent fluctuations in the flow on the scale of the event horizon.  Quantitative comparison of observed variability to the models is a potentially powerful tool for model selection.

Here we consider two measures of variability: one from the ALMA light curves and another based on variability of visibility amplitudes in EHT data.

Nearly all models fail the variability tests, in that the models fluctuate more than the data.  We will discuss possible reasons for this {\em variability crisis} in \S 5.

Although we provide a full discussion of model variability, we will not use it for model selection, under the notion that the variability is a higher-order - and therefore less well predicted -  feature of the models while lower order features, such as 86GHz flux density and 230GHz source geometry, are still well predicted.  This hypothesis can only be tested once models that accurately predicted variability are available.

%Compared to other astrophysical systems, optically thin black hole accretion systems such as \sgra are expected to show variabilities in a wide range of time scales, from the boundary conditions at the Bondi radius determined by the ``environment'', all the way to the turbulent fluctuations near the event horizon, or even plasma instability driven phenomena.
%Even excluding flares and detailed plasma physics, observations have shown intrinsic variability in \sgra~\citep{sgra lightcurve papers}.
%The standard theoretical interpretation is that the turbulence in the accretion flow drives the fluctuations in, e.g., temperature and magnetic fields, which then drive the fluctuations in the electromagnetic signals.
%Assuming that the magnetorotational instability (MRI) is the driving mechanism, the turbulence integrated time scale is simply the orbital time scale.
%Given that synchrotron radiation mainly comes from the inner accretion disk, this time scale is of order of 10 minutes.
%However, the chaotic nature of turbulence makes these short time scale variability difficult to predict.
%Therefore, we will not rule out models using short time scale variability.
%On the other hand, numerical simulations are very successful in predicting the slowly varying mean images, SEDs, etc because these are controlled by the conservation laws and the boundary conditions.
%This justifies the usage of mean and median quantities to constraint models described in the earlier sections.

%------------------------------------------------------------------------------
\subsubsection{ALMA Light curves}

\mw{I edited this a bit}

ALMA and SMA produced \sgra light curves at 230GHz as a byproduct of the 2017 EHT VLBI observing campaign. The complete set of light curves is presented and analyzed in \cite{Wielgus2022}. We have chosen to compare the models to the 7 April 2017 ALMA light using
the 3 hour {\em modulation index} $M_3$, where $M_T \equiv
\sigma/\mu$, $\sigma$ is the standard deviation measured over some
interval $T$, and $\mu$ is the mean measured over the same interval.

We use $M_T$ because it is easy to describe, easy to compute, commonly used in the literature (in the X-ray astronomy literature it is ``rms \%''), and closely related to the structure function, since the expectation value for $\sigma^2$ is given by an integral over the structure function (see Lee et al. 2021).

We use $T = 3$ hours because 3 hours is similar to the correlation time for $F_{230}$ in most of the models, 3 hours is similar to the characteristic timescales measured in damped random walk fits to the ALMA lightcurve \citep[see Table 10 of][]{Wielgus2022}, and 3 hours is the longest timescale for which we can consistently estimate the mean and variance of the distribution of M$_3$ from the models.  For a damped random walk process one can show that $M_3$ is only weakly correlated over successive 3 hour intervals (Lee et al. 2021).

We measure $M_3$ in maximally spaced intervals in the ALMA lightcurve on April 6, 7, and 11.\footnote{The results are not sensitive to interval spacing.}  We have also measured $M_3$ in historical data for \sgra from 2005-2017 light curves.  The April 6, 7, and 11 values for $M_3$ (5 in all) are consistent with having been drawn from the historical distribution, although April 7 is among the quietest intervals on record.

%The constraint comes from $M_T$ measured over 3 maximally spaced intervals
%in the 7 April 2017 ALMA light curve, where $M_3 = 0.024, 0.051,
%0.047$ (Wielgus et al. 2021). These values are consistent with being drawn from the distribution estimated from historical non-EHT 2005-2017 light curves.
%curves. \mw{maybe sth like "from the estimated distribution of the historical 2005-2017 lightcurves"?}

%------------------------------------------------------------------------------
\subsubsection{EHT (Baseline variability)}

\ckc{I think Boris needs to write this...}

Fluctuations in source structure are expected to lead to fluctuations in visibility amplitudes.  An accompanying paper  (Georgiev et al.) describes a technique for characterizing this variability by calculating the temporal power spectral density of ALMA-light-curve-normalized visibility amplitudes in a band in $uv$ distance around $4$G$\lambda$.

Applying this technique to the full 2017 EHT dataset (April 6, 7, and 11), the normalized PSD is $\approx 2 \times 10^{-2} \pm $.
Georgiev et al. also compute the normalized PSD for most of our models.
